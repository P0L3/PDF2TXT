from io import StringIO
from pdfminer.high_level import extract_text_to_fp
from pdfminer.layout import LAParams
import logging
import re
from nltk.corpus import words
import nltk
# nltk.download('words')
# nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
 
lemmatizer = WordNetLemmatizer()

ligatures_dict = [{'unicode': 'U+0132', 'sign': 'Ä²', 'norm': 'IJ', 'Ä²': 'IJ'}, {'unicode': 'U+0133', 'sign': 'Ä³', 'norm': 'ij', 'Ä³': 'ij'}, {'unicode': 'U+01C7', 'sign': 'Ç‡', 'norm': 'LJ', 'Ç‡': 'LJ'}, {'unicode': 'U+01C8', 'sign': 'Çˆ', 'norm': 'Lj', 'Çˆ': 'Lj'}, {'unicode': 'U+01C9', 'sign': 'Ç‰', 'norm': 'lj', 'Ç‰': 'lj'}, {'unicode': 'U+01CA', 'sign': 'ÇŠ', 'norm': 'NJ', 'ÇŠ': 'NJ'}, {'unicode': 'U+01CB', 'sign': 'Ç‹', 'norm': 'Nj', 'Ç‹': 'Nj'}, {'unicode': 'U+01CC', 'sign': 'ÇŒ', 'norm': 'nj', 'ÇŒ': 'nj'}, {'unicode': 'U+01F1', 'sign': 'Ç±', 'norm': 'DZ', 'Ç±': 'DZ'}, {'unicode': 'U+01F2', 'sign': 'Ç²', 'norm': 'Dz', 'Ç²': 'Dz'}, {'unicode': 'U+01F3', 'sign': 'Ç³', 'norm': 'dz', 'Ç³': 'dz'}, {'unicode': 'U+20A8', 'sign': 'â‚¨', 'norm': 'Rs', 'â‚¨': 'Rs'}, {'unicode': 'U+2116', 'sign': 'â„–', 'norm': 'No', 'â„–': 'No'}, {'unicode': 'U+2120', 'sign': 'â„ ', 'norm': 'SM', 'â„ ': 'SM'}, {'unicode': 'U+2121', 'sign': 'â„¡', 'norm': 'TEL', 'â„¡': 'TEL'}, {'unicode': 'U+2122', 'sign': 'â„¢', 'norm': 'TM', 'â„¢': 'TM'}, {'unicode': 'U+213B', 'sign': 'â„»', 'norm': 'FAX', 'â„»': 'FAX'}, {'unicode': 'U+2161', 'sign': 'â…¡', 'norm': 'II', 'â…¡': 'II'}, {'unicode': 'U+2162', 'sign': 'â…¢', 'norm': 'III', 'â…¢': 'III'}, {'unicode': 'U+2163', 'sign': 'â…£', 'norm': 'IV', 'â…£': 'IV'}, {'unicode': 'U+2165', 'sign': 'â…¥', 'norm': 'VI', 'â…¥': 'VI'}, {'unicode': 'U+2166', 'sign': 'â…¦', 'norm': 'VII', 'â…¦': 'VII'}, {'unicode': 'U+2167', 'sign': 'â…§', 'norm': 'VIII', 'â…§': 'VIII'}, {'unicode': 'U+2168', 'sign': 'â…¨', 'norm': 'IX', 'â…¨': 'IX'}, {'unicode': 'U+216A', 'sign': 'â…ª', 'norm': 'XI', 'â…ª': 'XI'}, {'unicode': 'U+216B', 'sign': 'â…«', 'norm': 'XII', 'â…«': 'XII'}, {'unicode': 'U+2171', 'sign': 'â…±', 'norm': 'ii', 'â…±': 'ii'}, {'unicode': 'U+2172', 'sign': 'â…²', 'norm': 'iii', 'â…²': 'iii'}, {'unicode': 'U+2173', 'sign': 'â…³', 'norm': 'iv', 'â…³': 'iv'}, {'unicode': 'U+2175', 'sign': 'â…µ', 'norm': 'vi', 'â…µ': 'vi'}, {'unicode': 'U+2176', 'sign': 'â…¶', 'norm': 'vii', 'â…¶': 'vii'}, {'unicode': 'U+2177', 'sign': 'â…·', 'norm': 'viii', 'â…·': 'viii'}, {'unicode': 'U+2178', 'sign': 'â…¸', 'norm': 'ix', 'â…¸': 'ix'}, {'unicode': 'U+217A', 'sign': 'â…º', 'norm': 'xi', 'â…º': 'xi'}, {'unicode': 'U+217B', 'sign': 'â…»', 'norm': 'xii', 'â…»': 'xii'}, {'unicode': 'U+3250', 'sign': 'ã‰', 'norm': 'PTE', 'ã‰': 'PTE'}, {'unicode': 'U+32CC', 'sign': 'ã‹Œ', 'norm': 'Hg', 'ã‹Œ': 'Hg'}, {'unicode': 'U+32CD', 'sign': 'ã‹', 'norm': 'erg', 'ã‹': 'erg'}, {'unicode': 'U+32CE', 'sign': 'ã‹', 'norm': 'eV', 'ã‹': 'eV'}, {'unicode': 'U+32CF', 'sign': 'ã‹', 'norm': 'LTD', 'ã‹': 'LTD'}, {'unicode': 'U+3371', 'sign': 'ã±', 'norm': 'hPa', 'ã±': 'hPa'}, {'unicode': 'U+3372', 'sign': 'ã²', 'norm': 'da', 'ã²': 'da'}, {'unicode': 'U+3373', 'sign': 'ã³', 'norm': 'AU', 'ã³': 'AU'}, {'unicode': 'U+3374', 'sign': 'ã´', 'norm': 'bar', 'ã´': 'bar'}, {'unicode': 'U+3375', 'sign': 'ãµ', 'norm': 'oV', 'ãµ': 'oV'}, {'unicode': 'U+3376', 'sign': 'ã¶', 'norm': 'pc', 'ã¶': 'pc'}, {'unicode': 'U+3377', 'sign': 'ã·', 'norm': 'dm', 'ã·': 'dm'}, {'unicode': 'U+337A', 'sign': 'ãº', 'norm': 'IU', 'ãº': 'IU'}, {'unicode': 'U+3380', 'sign': 'ã€', 'norm': 'pA', 'ã€': 'pA'}, {'unicode': 'U+3381', 'sign': 'ã', 'norm': 'nA', 'ã': 'nA'}, {'unicode': 'U+3383', 'sign': 'ãƒ', 'norm': 'mA', 'ãƒ': 'mA'}, {'unicode': 'U+3384', 'sign': 'ã„', 'norm': 'kA', 'ã„': 'kA'}, {'unicode': 'U+3385', 'sign': 'ã…', 'norm': 'KB', 'ã…': 'KB'}, {'unicode': 'U+3386', 'sign': 'ã†', 'norm': 'MB', 'ã†': 'MB'}, {'unicode': 'U+3387', 'sign': 'ã‡', 'norm': 'GB', 'ã‡': 'GB'}, {'unicode': 'U+3388', 'sign': 'ãˆ', 'norm': 'cal', 'ãˆ': 'cal'}, {'unicode': 'U+3389', 'sign': 'ã‰', 'norm': 'kcal', 'ã‰': 'kcal'}, {'unicode': 'U+338A', 'sign': 'ãŠ', 'norm': 'pF', 'ãŠ': 'pF'}, {'unicode': 'U+338B', 'sign': 'ã‹', 'norm': 'nF', 'ã‹': 'nF'}, {'unicode': 'U+338E', 'sign': 'ã', 'norm': 'mg', 'ã': 'mg'}, {'unicode': 'U+338F', 'sign': 'ã', 'norm': 'kg', 'ã': 'kg'}, {'unicode': 'U+3390', 'sign': 'ã', 'norm': 'Hz', 'ã': 'Hz'}, {'unicode': 'U+3391', 'sign': 'ã‘', 'norm': 'kHz', 'ã‘': 'kHz'}, {'unicode': 'U+3392', 'sign': 'ã’', 'norm': 'MHz', 'ã’': 'MHz'}, {'unicode': 'U+3393', 'sign': 'ã“', 'norm': 'GHz', 'ã“': 'GHz'}, {'unicode': 'U+3394', 'sign': 'ã”', 'norm': 'THz', 'ã”': 'THz'}, {'unicode': 'U+3396', 'sign': 'ã–', 'norm': 'ml', 'ã–': 'ml'}, {'unicode': 'U+3397', 'sign': 'ã—', 'norm': 'dl', 'ã—': 'dl'}, {'unicode': 'U+3398', 'sign': 'ã˜', 'norm': 'kl', 'ã˜': 'kl'}, {'unicode': 'U+3399', 'sign': 'ã™', 'norm': 'fm', 'ã™': 'fm'}, {'unicode': 'U+339A', 'sign': 'ãš', 'norm': 'nm', 'ãš': 'nm'}, {'unicode': 'U+339C', 'sign': 'ãœ', 'norm': 'mm', 'ãœ': 'mm'}, {'unicode': 'U+339D', 'sign': 'ã', 'norm': 'cm', 'ã': 'cm'}, {'unicode': 'U+339E', 'sign': 'ã', 'norm': 'km', 'ã': 'km'}, {'unicode': 'U+33A9', 'sign': 'ã©', 'norm': 'Pa', 'ã©': 'Pa'}, {'unicode': 'U+33AA', 'sign': 'ãª', 'norm': 'kPa', 'ãª': 'kPa'}, {'unicode': 'U+33AB', 'sign': 'ã«', 'norm': 'MPa', 'ã«': 'MPa'}, {'unicode': 'U+33AC', 'sign': 'ã¬', 'norm': 'GPa', 'ã¬': 'GPa'}, {'unicode': 'U+33AD', 'sign': 'ã­', 'norm': 'rad', 'ã­': 'rad'}, {'unicode': 'U+33B0', 'sign': 'ã°', 'norm': 'ps', 'ã°': 'ps'}, {'unicode': 'U+33B1', 'sign': 'ã±', 'norm': 'ns', 'ã±': 'ns'}, {'unicode': 'U+33B3', 'sign': 'ã³', 'norm': 'ms', 'ã³': 'ms'}, {'unicode': 'U+33B4', 'sign': 'ã´', 'norm': 'pV', 'ã´': 'pV'}, {'unicode': 'U+33B5', 'sign': 'ãµ', 'norm': 'nV', 'ãµ': 'nV'}, {'unicode': 'U+33B7', 'sign': 'ã·', 'norm': 'mV', 'ã·': 'mV'}, {'unicode': 'U+33B8', 'sign': 'ã¸', 'norm': 'kV', 'ã¸': 'kV'}, {'unicode': 'U+33B9', 'sign': 'ã¹', 'norm': 'MV', 'ã¹': 'MV'}, {'unicode': 'U+33BA', 'sign': 'ãº', 'norm': 'pW', 'ãº': 'pW'}, {'unicode': 'U+33BB', 'sign': 'ã»', 'norm': 'nW', 'ã»': 'nW'}, {'unicode': 'U+33BD', 'sign': 'ã½', 'norm': 'mW', 'ã½': 'mW'}, {'unicode': 'U+33BE', 'sign': 'ã¾', 'norm': 'kW', 'ã¾': 'kW'}, {'unicode': 'U+33BF', 'sign': 'ã¿', 'norm': 'MW', 'ã¿': 'MW'}, {'unicode': 'U+33C3', 'sign': 'ãƒ', 'norm': 'Bq', 'ãƒ': 'Bq'}, {'unicode': 'U+33C4', 'sign': 'ã„', 'norm': 'cc', 'ã„': 'cc'}, {'unicode': 'U+33C5', 'sign': 'ã…', 'norm': 'cd', 'ã…': 'cd'}, {'unicode': 'U+33C8', 'sign': 'ãˆ', 'norm': 'dB', 'ãˆ': 'dB'}, {'unicode': 'U+33C9', 'sign': 'ã‰', 'norm': 'Gy', 'ã‰': 'Gy'}, {'unicode': 'U+33CA', 'sign': 'ãŠ', 'norm': 'ha', 'ãŠ': 'ha'}, {'unicode': 'U+33CB', 'sign': 'ã‹', 'norm': 'HP', 'ã‹': 'HP'}, {'unicode': 'U+33CC', 'sign': 'ãŒ', 'norm': 'in', 'ãŒ': 'in'}, {'unicode': 'U+33CD', 'sign': 'ã', 'norm': 'KK', 'ã': 'KK'}, {'unicode': 'U+33CE', 'sign': 'ã', 'norm': 'KM', 'ã': 'KM'}, {'unicode': 'U+33CF', 'sign': 'ã', 'norm': 'kt', 'ã': 'kt'}, {'unicode': 'U+33D0', 'sign': 'ã', 'norm': 'lm', 'ã': 'lm'}, {'unicode': 'U+33D1', 'sign': 'ã‘', 'norm': 'ln', 'ã‘': 'ln'}, {'unicode': 'U+33D2', 'sign': 'ã’', 'norm': 'log', 'ã’': 'log'}, {'unicode': 'U+33D3', 'sign': 'ã“', 'norm': 'lx', 'ã“': 'lx'}, {'unicode': 'U+33D4', 'sign': 'ã”', 'norm': 'mb', 'ã”': 'mb'}, {'unicode': 'U+33D5', 'sign': 'ã•', 'norm': 'mil', 'ã•': 'mil'}, {'unicode': 'U+33D6', 'sign': 'ã–', 'norm': 'mol', 'ã–': 'mol'}, {'unicode': 'U+33D7', 'sign': 'ã—', 'norm': 'PH', 'ã—': 'PH'}, {'unicode': 'U+33D9', 'sign': 'ã™', 'norm': 'PPM', 'ã™': 'PPM'}, {'unicode': 'U+33DA', 'sign': 'ãš', 'norm': 'PR', 'ãš': 'PR'}, {'unicode': 'U+33DB', 'sign': 'ã›', 'norm': 'sr', 'ã›': 'sr'}, {'unicode': 'U+33DC', 'sign': 'ãœ', 'norm': 'Sv', 'ãœ': 'Sv'}, {'unicode': 'U+33DD', 'sign': 'ã', 'norm': 'Wb', 'ã': 'Wb'}, {'unicode': 'U+33FF', 'sign': 'ã¿', 'norm': 'gal', 'ã¿': 'gal'}, {'unicode': 'U+FB00', 'sign': 'ï¬€', 'norm': 'ff', 'ï¬€': 'ff'}, {'unicode': 'U+FB01', 'sign': 'ï¬', 'norm': 'fi', 'ï¬': 'fi'}, {'unicode': 'U+FB02', 'sign': 'ï¬‚', 'norm': 'fl', 'ï¬‚': 'fl'}, {'unicode': 'U+FB03', 'sign': 'ï¬ƒ', 'norm': 'ffi', 'ï¬ƒ': 'ffi'}, {'unicode': 'U+FB04', 'sign': 'ï¬„', 'norm': 'ffl', 'ï¬„': 'ffl'}, {'unicode': 'U+FB05', 'sign': 'ï¬…', 'norm': 'st', 'ï¬…': 'st'}, {'unicode': 'U+FB06', 'sign': 'ï¬†', 'norm': 'st', 'ï¬†': 'st'}, {'unicode': 'U+1F12D', 'sign': 'ğŸ„­', 'norm': 'CD', 'ğŸ„­': 'CD'}, {'unicode': 'U+1F12E', 'sign': 'ğŸ„®', 'norm': 'WZ', 'ğŸ„®': 'WZ'}, {'unicode': 'U+1F14A', 'sign': 'ğŸ…Š', 'norm': 'HV', 'ğŸ…Š': 'HV'}, {'unicode': 'U+1F14B', 'sign': 'ğŸ…‹', 'norm': 'MV', 'ğŸ…‹': 'MV'}, {'unicode': 'U+1F14C', 'sign': 'ğŸ…Œ', 'norm': 'SD', 'ğŸ…Œ': 'SD'}, {'unicode': 'U+1F14D', 'sign': 'ğŸ…', 'norm': 'SS', 'ğŸ…': 'SS'}, {'unicode': 'U+1F14E', 'sign': 'ğŸ…', 'norm': 'PPV', 'ğŸ…': 'PPV'}, {'unicode': 'U+1F14F', 'sign': 'ğŸ…', 'norm': 'WC', 'ğŸ…': 'WC'}, {'unicode': 'U+1F16A', 'sign': 'ğŸ…ª', 'norm': 'MC', 'ğŸ…ª': 'MC'}, {'unicode': 'U+1F16B', 'sign': 'ğŸ…«', 'norm': 'MD', 'ğŸ…«': 'MD'}, {'unicode': 'U+1F16C', 'sign': 'ğŸ…¬', 'norm': 'MR', 'ğŸ…¬': 'MR'}, {'unicode': 'U+1F190', 'sign': 'ğŸ†', 'norm': 'DJ', 'ğŸ†': 'DJ'}]
ligatures_conv = {'Ä²': 'IJ', 'Ä³': 'ij', 'Ç‡': 'LJ', 'Çˆ': 'Lj', 'Ç‰': 'lj', 'ÇŠ': 'NJ', 'Ç‹': 'Nj', 'ÇŒ': 'nj', 'Ç±': 'DZ', 'Ç²': 'Dz', 'Ç³': 'dz', 'â‚¨': 'Rs', 'â„–': 'No', 'â„ ': 'SM', 'â„¡': 'TEL', 'â„¢': 'TM', 'â„»': 'FAX', 'â…¡': 'II', 'â…¢': 'III', 'â…£': 'IV', 'â…¥': 'VI', 'â…¦': 'VII', 'â…§': 'VIII', 'â…¨': 'IX', 'â…ª': 'XI', 'â…«': 'XII', 'â…±': 'ii', 'â…²': 'iii', 'â…³': 'iv', 'â…µ': 'vi', 'â…¶': 'vii', 'â…·': 'viii', 'â…¸': 'ix', 'â…º': 'xi', 'â…»': 'xii', 'ã‰': 'PTE', 'ã‹Œ': 'Hg', 'ã‹': 'erg', 'ã‹': 'eV', 'ã‹': 'LTD', 'ã±': 'hPa', 'ã²': 'da', 'ã³': 'AU', 'ã´': 'bar', 'ãµ': 'oV', 'ã¶': 'pc', 'ã·': 'dm', 'ãº': 'IU', 'ã€': 'pA', 'ã': 'nA', 'ãƒ': 'mA', 'ã„': 'kA', 'ã…': 'KB', 'ã†': 'MB', 'ã‡': 'GB', 'ãˆ': 'cal', 'ã‰': 'kcal', 'ãŠ': 'pF', 'ã‹': 'nF', 'ã': 'mg', 'ã': 'kg', 'ã': 'Hz', 'ã‘': 'kHz', 'ã’': 'MHz', 'ã“': 'GHz', 'ã”': 'THz', 'ã–': 'ml', 'ã—': 'dl', 'ã˜': 'kl', 'ã™': 'fm', 'ãš': 'nm', 'ãœ': 'mm', 'ã': 'cm', 'ã': 'km', 'ã©': 'Pa', 'ãª': 'kPa', 'ã«': 'MPa', 'ã¬': 'GPa', 'ã­': 'rad', 'ã°': 'ps', 'ã±': 'ns', 'ã³': 'ms', 'ã´': 'pV', 'ãµ': 'nV', 'ã·': 'mV', 'ã¸': 'kV', 'ã¹': 'MV', 'ãº': 'pW', 'ã»': 'nW', 'ã½': 'mW', 'ã¾': 'kW', 'ã¿': 'MW', 'ãƒ': 'Bq', 'ã„': 'cc', 'ã…': 'cd', 'ãˆ': 'dB', 'ã‰': 'Gy', 'ãŠ': 'ha', 'ã‹': 'HP', 'ãŒ': 'in', 'ã': 'KK', 'ã': 'KM', 'ã': 'kt', 'ã': 'lm', 'ã‘': 'ln', 'ã’': 'log', 'ã“': 'lx', 'ã”': 'mb', 'ã•': 'mil', 'ã–': 'mol', 'ã—': 'PH', 'ã™': 'PPM', 'ãš': 'PR', 'ã›': 'sr', 'ãœ': 'Sv', 'ã': 'Wb', 'ã¿': 'gal', 'ï¬€': 'ff', 'ï¬': 'fi', 'ï¬‚': 'fl', 'ï¬ƒ': 'ffi', 'ï¬„': 'ffl', 'ï¬…': 'st', 'ï¬†': 'st', 'ğŸ„­': 'CD', 'ğŸ„®': 'WZ', 'ğŸ…Š': 'HV', 'ğŸ…‹': 'MV', 'ğŸ…Œ': 'SD', 'ğŸ…': 'SS', 'ğŸ…': 'PPV', 'ğŸ…': 'WC', 'ğŸ…ª': 'MC', 'ğŸ…«': 'MD', 'ğŸ…¬': 'MR', 'ğŸ†': 'DJ'}
ligatures_list = ['Ä²', 'Ä³', 'Ç‡', 'Çˆ', 'Ç‰', 'ÇŠ', 'Ç‹', 'ÇŒ', 'Ç±', 'Ç²', 'Ç³', 'â‚¨', 'â„–', 'â„ ', 'â„¡', 'â„¢', 'â„»', 'â…¡', 'â…¢', 'â…£', 'â…¥', 'â…¦', 'â…§', 'â…¨', 'â…ª', 'â…«', 'â…±', 'â…²', 'â…³', 'â…µ', 'â…¶', 'â…·', 'â…¸', 'â…º', 'â…»', 'ã‰', 'ã‹Œ', 'ã‹', 'ã‹', 'ã‹', 'ã±', 'ã²', 'ã³', 'ã´', 'ãµ', 'ã¶', 'ã·', 'ãº', 'ã€', 'ã', 'ãƒ', 'ã„', 'ã…', 'ã†', 'ã‡', 'ãˆ', 'ã‰', 'ãŠ', 'ã‹', 'ã', 'ã', 'ã', 'ã‘', 'ã’', 'ã“', 'ã”', 'ã–', 'ã—', 'ã˜', 'ã™', 'ãš', 'ãœ', 'ã', 'ã', 'ã©', 'ãª', 'ã«', 'ã¬', 'ã­', 'ã°', 'ã±', 'ã³', 'ã´', 'ãµ', 'ã·', 'ã¸', 'ã¹', 'ãº', 'ã»', 'ã½', 'ã¾', 'ã¿', 'ãƒ', 'ã„', 'ã…', 'ãˆ', 'ã‰', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã', 'ã', 'ã‘', 'ã’', 'ã“', 'ã”', 'ã•', 'ã–', 'ã—', 'ã™', 'ãš', 'ã›', 'ãœ', 'ã', 'ã¿', 'ï¬€', 'ï¬', 'ï¬‚', 'ï¬ƒ', 'ï¬„', 'ï¬…', 'ï¬†', 'ğŸ„­', 'ğŸ„®', 'ğŸ…Š', 'ğŸ…‹', 'ğŸ…Œ', 'ğŸ…', 'ğŸ…', 'ğŸ…', 'ğŸ…ª', 'ğŸ…«', 'ğŸ…¬', 'ğŸ†']
pattern = "Ä²|Ä³|Ç‡|Çˆ|Ç‰|ÇŠ|Ç‹|ÇŒ|Ç±|Ç²|Ç³|â‚¨|â„–|â„ |â„¡|â„¢|â„»|â…¡|â…¢|â…£|â…¥|â…¦|â…§|â…¨|â…ª|â…«|â…±|â…²|â…³|â…µ|â…¶|â…·|â…¸|â…º|â…»|ã‰|ã‹Œ|ã‹|ã‹|ã‹|ã±|ã²|ã³|ã´|ãµ|ã¶|ã·|ãº|ã€|ã|ãƒ|ã„|ã…|ã†|ã‡|ãˆ|ã‰|ãŠ|ã‹|ã|ã|ã|ã‘|ã’|ã“|ã”|ã–|ã—|ã˜|ã™|ãš|ãœ|ã|ã|ã©|ãª|ã«|ã¬|ã­|ã°|ã±|ã³|ã´|ãµ|ã·|ã¸|ã¹|ãº|ã»|ã½|ã¾|ã¿|ãƒ|ã„|ã…|ãˆ|ã‰|ãŠ|ã‹|ãŒ|ã|ã|ã|ã|ã‘|ã’|ã“|ã”|ã•|ã–|ã—|ã™|ãš|ã›|ãœ|ã|ã¿|ï¬€|ï¬|ï¬‚|ï¬ƒ|ï¬„|ï¬…|ï¬†|ğŸ„­|ğŸ„®|ğŸ…Š|ğŸ…‹|ğŸ…Œ|ğŸ…|ğŸ…|ğŸ…|ğŸ…ª|ğŸ…«|ğŸ…¬|ğŸ†"



def pdf2html(target="./SAMPLE/NCLIMATE/s41558-020-00938-y_Heat_Tolerance_In_Ectotherms_Scales_Predictably_With_Body_Size_.pdf", line_margin=0.5, all_texts=True):
    """
    Converts a PDF file to HTML format.

    Args:
        target (str): Path to the PDF file to be converted. Default is "./SAMPLE/NCLIMATE/s41558-020-00938-y_Heat_Tolerance_In_Ectotherms_Scales_Predictably_With_Body_Size_.pdf".
        line_margin (float): Margin between lines in the output HTML. Default is 0.5.
        all_texts (bool): If True, extracts all text from the PDF. If False, only extracts text from visible elements. Default is True.

    Returns:
        str: HTML representation of the PDF content.

    Raises:
        None.

    Note:
        This function utilizes the `extract_text_to_fp` method from PyPDF2 library to extract text from the PDF.
        If the PDF cannot be read, a warning message is logged, and None is returned.

    """

    try:
        output_string = StringIO()

        with open(target, 'rb') as fin:
            extract_text_to_fp(fin, output_string, laparams=LAParams(), output_type='html', codec=None)

        return output_string.getvalue()
    except:
        warning_message = f"Unable to read PDF. Skipping..."
        logging.warning(warning_message)
        return None
    
def check_if_ium(soup):
    """
    Check if the provided BeautifulSoup 'soup' object indicates incomplete Unicode mappings (ium).

    Parameters:
    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML content.

    Returns:
    - bool: True if incomplete Unicode mappings are found, False otherwise.

    The function searches for a 'div' element in the provided 'soup' object and iterates through its
    next siblings until a text length of at least 50 characters is found or no more siblings are present.
    It then uses a regular expression to check if the text contains at least three consecutive instances
    of the pattern "(cid:\d+)" which might indicate incomplete Unicode mappings.
    """
    elem = soup.find("div")
    while len(elem.text) < 50 and not type(elem) == type(None):
        elem = elem.find_next()

    if type(elem) == type(None):
        raise Exception("NO div element found!")
    
    return bool(re.search("(\(cid:\d+\)){3,}", elem.text))

def add_custom_tag_after_element(soup, target_element, tag_name, tag_content, style_attributes):
    """
    Add a custom tag with specified content after a specific element in the BeautifulSoup 'soup' object.

    Parameters:
    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML content.
    - target_element (Tag): The target element after which the new tag will be added.
    - tag_name (str): The name of the custom tag to be added.
    - tag_content (str): The content to be placed inside the custom tag.

    Returns:
    - BeautifulSoup: The modified BeautifulSoup object.
    """
    if type(target_element) == type(None):
        warning_message = "Tag is not added correctly -> Implies that the target element wasn't found correctly prior"
        logging.warning(warning_message)
        return soup
    new_tag = soup.new_tag(tag_name)
    new_tag.string = tag_content
    new_tag.attrs.update(style_attributes)
    target_element.insert_after(new_tag)
    return soup

def find_custom_element_by_regex(soup, regex="^(?i)r\s*e\s*f\s*e\s*r\s*e\s*n\s*c\s*e\s*s\n+", reverese=True):
    """
    Finds a custom HTML element within a BeautifulSoup object based on a given regular expression.

    Args:
    - soup: BeautifulSoup object representing HTML content.
    - regex (str): Regular expression pattern to search for within the text content of HTML elements.
    - reverse (bool): If True, searches for the element in reverse order (from the last element to the first).

    Returns:
    - elem: The BeautifulSoup element that matches the specified regex pattern, or None if not found.
    """
    if reverese:
        elem = soup.find_all('div')[-1]
        while type(elem) != type(None):
            if re.search(regex, elem.text):   
                # print(elem.text)
                break
            elem = elem.find_previous()
    else:
        elem = soup.find('div')
        while type(elem) != type(None):
            if re.search(regex, elem.text):   
                # print(elem.text)
                break
            elem = elem.find_next()

    return elem

def likely_word(tokenbefore, token, tokenafter):
    """
    Check if the combination of tokens (current, before, and after) constitutes a likely word.

    Args:
        tokenbefore (str): The token preceding the current token.
        token (str): The current token.
        tokenafter (str): The token following the current token.

    Returns:
        tuple: A tuple containing the following elements:
            - str: The modified tokenbefore string, after cleaning and processing.
            - str: The modified token string, after cleaning and processing.
            - str: The modified tokenafter string, after cleaning and processing.
            - int: Flag indicating if a likely word is found (1) or not (0).
    """

    temp_token = token
    temp_tokenbefore = tokenbefore
    temp_tokenafter = tokenafter

    # Clean from punctuation and simmilar
    token = re.sub(r"[(),.!?;]+", "", token).lower()
    tokenbefore = re.sub(r"[(),.!?;]+", "", tokenbefore).lower()
    tokenafter = re.sub(r"[(),.!?;]+", "", tokenafter).lower()

    # Deal with concatenated words, such as "age-specific"
    tokenbefore = re.sub(r"\w+-", "", tokenbefore)

    replace_token = ""
    
    if tokenbefore+token+tokenafter in words.words() or lemmatizer.lemmatize(tokenbefore+token+tokenafter) in words.words() or lemmatizer.lemmatize(tokenbefore+token+tokenafter, pos='v') in words.words():
        return replace_token, temp_tokenbefore+temp_token+temp_tokenafter, replace_token, 1
    
    elif token+tokenafter in words.words() or lemmatizer.lemmatize(token+tokenafter) in words.words() or lemmatizer.lemmatize(token+tokenafter, pos='v') in words.words():
        return temp_tokenbefore, temp_token+temp_tokenafter, replace_token, 1
    
    elif tokenbefore+token in words.words() or lemmatizer.lemmatize(tokenbefore+token) in words.words() or lemmatizer.lemmatize(tokenbefore+token, pos='v') in words.words():
        return replace_token, temp_tokenbefore+temp_token, temp_tokenafter, 1
    
    elif token in words.words() in words.words():
        return temp_tokenbefore, temp_token, temp_tokenafter, 1
    
    else:
        if len(temp_token) > 2:
            return temp_tokenbefore, temp_token, temp_tokenafter, 1
        else: 
            # print(temp_tokenbefore, temp_token, temp_tokenafter)
            return temp_tokenbefore, replace_token, temp_tokenafter, 0
        
def fi_cleaner(text):
    """
    Cleans the text by replacing ligatures with their corresponding characters.
    
    Args:
    text (str): The input text to be cleaned.
    
    Returns:
    str: The cleaned text with ligatures replaced.
    """
    
    tokens = text.split()
    
    for i, word in enumerate(tokens):
        count = 0

        for j, c in enumerate(word):
            if c in ligatures_list:
                if (j == 0 or j == len(word)-1):
                    # temp = tokens[i]
                    # print(20*"-")
                    # print(i, "\t", tokens[i-1], tokens[i], tokens[i+1], end="----")
                    count += 1
                    try:
                        tokens[i-1], tokens[i], tokens[i+1], f = likely_word(tokens[i-1], re.sub(pattern, lambda m: ligatures_conv.get(m.group(0)), tokens[i]), tokens[i+1])
                    except:
                        continue
                    # if f == 0:
                    #     fi_counter_unsolved.append((temp, tokens[i]))
                    # else:
                    #     fi_counter_solved.append((temp, tokens[i]))
                else:
                    tokens[i] = re.sub(pattern, lambda m: ligatures_conv.get(m.group(0)), tokens[i])
                    count += 1
                # print(tokens[i-1], tokens[i], tokens[i+1])
        if count > 1: # Check if some word containes multiple ligatures
            print(i, "\t", tokens[i-1], tokens[i], tokens[i+1])
            warning_message = f"Multiple ligatures in single word!"
            logging.warning(warning_message)
            
    return " ".join(tokens)
    
