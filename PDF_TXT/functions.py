from io import StringIO
from pdfminer.high_level import extract_text_to_fp
from pdfminer.layout import LAParams
import logging
import re
from nltk.corpus import words
import nltk
# nltk.download('words')
# nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
 
lemmatizer = WordNetLemmatizer()

ligatures_dict = [{'unicode': 'U+0132', 'sign': 'ƒ≤', 'norm': 'IJ', 'ƒ≤': 'IJ'}, {'unicode': 'U+0133', 'sign': 'ƒ≥', 'norm': 'ij', 'ƒ≥': 'ij'}, {'unicode': 'U+01C7', 'sign': '«á', 'norm': 'LJ', '«á': 'LJ'}, {'unicode': 'U+01C8', 'sign': '«à', 'norm': 'Lj', '«à': 'Lj'}, {'unicode': 'U+01C9', 'sign': '«â', 'norm': 'lj', '«â': 'lj'}, {'unicode': 'U+01CA', 'sign': '«ä', 'norm': 'NJ', '«ä': 'NJ'}, {'unicode': 'U+01CB', 'sign': '«ã', 'norm': 'Nj', '«ã': 'Nj'}, {'unicode': 'U+01CC', 'sign': '«å', 'norm': 'nj', '«å': 'nj'}, {'unicode': 'U+01F1', 'sign': '«±', 'norm': 'DZ', '«±': 'DZ'}, {'unicode': 'U+01F2', 'sign': '«≤', 'norm': 'Dz', '«≤': 'Dz'}, {'unicode': 'U+01F3', 'sign': '«≥', 'norm': 'dz', '«≥': 'dz'}, {'unicode': 'U+20A8', 'sign': '‚Ç®', 'norm': 'Rs', '‚Ç®': 'Rs'}, {'unicode': 'U+2116', 'sign': '‚Ññ', 'norm': 'No', '‚Ññ': 'No'}, {'unicode': 'U+2120', 'sign': '‚Ñ†', 'norm': 'SM', '‚Ñ†': 'SM'}, {'unicode': 'U+2121', 'sign': '‚Ñ°', 'norm': 'TEL', '‚Ñ°': 'TEL'}, {'unicode': 'U+2122', 'sign': '‚Ñ¢', 'norm': 'TM', '‚Ñ¢': 'TM'}, {'unicode': 'U+213B', 'sign': '‚Ñª', 'norm': 'FAX', '‚Ñª': 'FAX'}, {'unicode': 'U+2161', 'sign': '‚Ö°', 'norm': 'II', '‚Ö°': 'II'}, {'unicode': 'U+2162', 'sign': '‚Ö¢', 'norm': 'III', '‚Ö¢': 'III'}, {'unicode': 'U+2163', 'sign': '‚Ö£', 'norm': 'IV', '‚Ö£': 'IV'}, {'unicode': 'U+2165', 'sign': '‚Ö•', 'norm': 'VI', '‚Ö•': 'VI'}, {'unicode': 'U+2166', 'sign': '‚Ö¶', 'norm': 'VII', '‚Ö¶': 'VII'}, {'unicode': 'U+2167', 'sign': '‚Öß', 'norm': 'VIII', '‚Öß': 'VIII'}, {'unicode': 'U+2168', 'sign': '‚Ö®', 'norm': 'IX', '‚Ö®': 'IX'}, {'unicode': 'U+216A', 'sign': '‚Ö™', 'norm': 'XI', '‚Ö™': 'XI'}, {'unicode': 'U+216B', 'sign': '‚Ö´', 'norm': 'XII', '‚Ö´': 'XII'}, {'unicode': 'U+2171', 'sign': '‚Ö±', 'norm': 'ii', '‚Ö±': 'ii'}, {'unicode': 'U+2172', 'sign': '‚Ö≤', 'norm': 'iii', '‚Ö≤': 'iii'}, {'unicode': 'U+2173', 'sign': '‚Ö≥', 'norm': 'iv', '‚Ö≥': 'iv'}, {'unicode': 'U+2175', 'sign': '‚Öµ', 'norm': 'vi', '‚Öµ': 'vi'}, {'unicode': 'U+2176', 'sign': '‚Ö∂', 'norm': 'vii', '‚Ö∂': 'vii'}, {'unicode': 'U+2177', 'sign': '‚Ö∑', 'norm': 'viii', '‚Ö∑': 'viii'}, {'unicode': 'U+2178', 'sign': '‚Ö∏', 'norm': 'ix', '‚Ö∏': 'ix'}, {'unicode': 'U+217A', 'sign': '‚Ö∫', 'norm': 'xi', '‚Ö∫': 'xi'}, {'unicode': 'U+217B', 'sign': '‚Öª', 'norm': 'xii', '‚Öª': 'xii'}, {'unicode': 'U+3250', 'sign': '„âê', 'norm': 'PTE', '„âê': 'PTE'}, {'unicode': 'U+32CC', 'sign': '„ãå', 'norm': 'Hg', '„ãå': 'Hg'}, {'unicode': 'U+32CD', 'sign': '„ãç', 'norm': 'erg', '„ãç': 'erg'}, {'unicode': 'U+32CE', 'sign': '„ãé', 'norm': 'eV', '„ãé': 'eV'}, {'unicode': 'U+32CF', 'sign': '„ãè', 'norm': 'LTD', '„ãè': 'LTD'}, {'unicode': 'U+3371', 'sign': '„ç±', 'norm': 'hPa', '„ç±': 'hPa'}, {'unicode': 'U+3372', 'sign': '„ç≤', 'norm': 'da', '„ç≤': 'da'}, {'unicode': 'U+3373', 'sign': '„ç≥', 'norm': 'AU', '„ç≥': 'AU'}, {'unicode': 'U+3374', 'sign': '„ç¥', 'norm': 'bar', '„ç¥': 'bar'}, {'unicode': 'U+3375', 'sign': '„çµ', 'norm': 'oV', '„çµ': 'oV'}, {'unicode': 'U+3376', 'sign': '„ç∂', 'norm': 'pc', '„ç∂': 'pc'}, {'unicode': 'U+3377', 'sign': '„ç∑', 'norm': 'dm', '„ç∑': 'dm'}, {'unicode': 'U+337A', 'sign': '„ç∫', 'norm': 'IU', '„ç∫': 'IU'}, {'unicode': 'U+3380', 'sign': '„éÄ', 'norm': 'pA', '„éÄ': 'pA'}, {'unicode': 'U+3381', 'sign': '„éÅ', 'norm': 'nA', '„éÅ': 'nA'}, {'unicode': 'U+3383', 'sign': '„éÉ', 'norm': 'mA', '„éÉ': 'mA'}, {'unicode': 'U+3384', 'sign': '„éÑ', 'norm': 'kA', '„éÑ': 'kA'}, {'unicode': 'U+3385', 'sign': '„éÖ', 'norm': 'KB', '„éÖ': 'KB'}, {'unicode': 'U+3386', 'sign': '„éÜ', 'norm': 'MB', '„éÜ': 'MB'}, {'unicode': 'U+3387', 'sign': '„éá', 'norm': 'GB', '„éá': 'GB'}, {'unicode': 'U+3388', 'sign': '„éà', 'norm': 'cal', '„éà': 'cal'}, {'unicode': 'U+3389', 'sign': '„éâ', 'norm': 'kcal', '„éâ': 'kcal'}, {'unicode': 'U+338A', 'sign': '„éä', 'norm': 'pF', '„éä': 'pF'}, {'unicode': 'U+338B', 'sign': '„éã', 'norm': 'nF', '„éã': 'nF'}, {'unicode': 'U+338E', 'sign': '„éé', 'norm': 'mg', '„éé': 'mg'}, {'unicode': 'U+338F', 'sign': '„éè', 'norm': 'kg', '„éè': 'kg'}, {'unicode': 'U+3390', 'sign': '„éê', 'norm': 'Hz', '„éê': 'Hz'}, {'unicode': 'U+3391', 'sign': '„éë', 'norm': 'kHz', '„éë': 'kHz'}, {'unicode': 'U+3392', 'sign': '„éí', 'norm': 'MHz', '„éí': 'MHz'}, {'unicode': 'U+3393', 'sign': '„éì', 'norm': 'GHz', '„éì': 'GHz'}, {'unicode': 'U+3394', 'sign': '„éî', 'norm': 'THz', '„éî': 'THz'}, {'unicode': 'U+3396', 'sign': '„éñ', 'norm': 'ml', '„éñ': 'ml'}, {'unicode': 'U+3397', 'sign': '„éó', 'norm': 'dl', '„éó': 'dl'}, {'unicode': 'U+3398', 'sign': '„éò', 'norm': 'kl', '„éò': 'kl'}, {'unicode': 'U+3399', 'sign': '„éô', 'norm': 'fm', '„éô': 'fm'}, {'unicode': 'U+339A', 'sign': '„éö', 'norm': 'nm', '„éö': 'nm'}, {'unicode': 'U+339C', 'sign': '„éú', 'norm': 'mm', '„éú': 'mm'}, {'unicode': 'U+339D', 'sign': '„éù', 'norm': 'cm', '„éù': 'cm'}, {'unicode': 'U+339E', 'sign': '„éû', 'norm': 'km', '„éû': 'km'}, {'unicode': 'U+33A9', 'sign': '„é©', 'norm': 'Pa', '„é©': 'Pa'}, {'unicode': 'U+33AA', 'sign': '„é™', 'norm': 'kPa', '„é™': 'kPa'}, {'unicode': 'U+33AB', 'sign': '„é´', 'norm': 'MPa', '„é´': 'MPa'}, {'unicode': 'U+33AC', 'sign': '„é¨', 'norm': 'GPa', '„é¨': 'GPa'}, {'unicode': 'U+33AD', 'sign': '„é≠', 'norm': 'rad', '„é≠': 'rad'}, {'unicode': 'U+33B0', 'sign': '„é∞', 'norm': 'ps', '„é∞': 'ps'}, {'unicode': 'U+33B1', 'sign': '„é±', 'norm': 'ns', '„é±': 'ns'}, {'unicode': 'U+33B3', 'sign': '„é≥', 'norm': 'ms', '„é≥': 'ms'}, {'unicode': 'U+33B4', 'sign': '„é¥', 'norm': 'pV', '„é¥': 'pV'}, {'unicode': 'U+33B5', 'sign': '„éµ', 'norm': 'nV', '„éµ': 'nV'}, {'unicode': 'U+33B7', 'sign': '„é∑', 'norm': 'mV', '„é∑': 'mV'}, {'unicode': 'U+33B8', 'sign': '„é∏', 'norm': 'kV', '„é∏': 'kV'}, {'unicode': 'U+33B9', 'sign': '„éπ', 'norm': 'MV', '„éπ': 'MV'}, {'unicode': 'U+33BA', 'sign': '„é∫', 'norm': 'pW', '„é∫': 'pW'}, {'unicode': 'U+33BB', 'sign': '„éª', 'norm': 'nW', '„éª': 'nW'}, {'unicode': 'U+33BD', 'sign': '„éΩ', 'norm': 'mW', '„éΩ': 'mW'}, {'unicode': 'U+33BE', 'sign': '„éæ', 'norm': 'kW', '„éæ': 'kW'}, {'unicode': 'U+33BF', 'sign': '„éø', 'norm': 'MW', '„éø': 'MW'}, {'unicode': 'U+33C3', 'sign': '„èÉ', 'norm': 'Bq', '„èÉ': 'Bq'}, {'unicode': 'U+33C4', 'sign': '„èÑ', 'norm': 'cc', '„èÑ': 'cc'}, {'unicode': 'U+33C5', 'sign': '„èÖ', 'norm': 'cd', '„èÖ': 'cd'}, {'unicode': 'U+33C8', 'sign': '„èà', 'norm': 'dB', '„èà': 'dB'}, {'unicode': 'U+33C9', 'sign': '„èâ', 'norm': 'Gy', '„èâ': 'Gy'}, {'unicode': 'U+33CA', 'sign': '„èä', 'norm': 'ha', '„èä': 'ha'}, {'unicode': 'U+33CB', 'sign': '„èã', 'norm': 'HP', '„èã': 'HP'}, {'unicode': 'U+33CC', 'sign': '„èå', 'norm': 'in', '„èå': 'in'}, {'unicode': 'U+33CD', 'sign': '„èç', 'norm': 'KK', '„èç': 'KK'}, {'unicode': 'U+33CE', 'sign': '„èé', 'norm': 'KM', '„èé': 'KM'}, {'unicode': 'U+33CF', 'sign': '„èè', 'norm': 'kt', '„èè': 'kt'}, {'unicode': 'U+33D0', 'sign': '„èê', 'norm': 'lm', '„èê': 'lm'}, {'unicode': 'U+33D1', 'sign': '„èë', 'norm': 'ln', '„èë': 'ln'}, {'unicode': 'U+33D2', 'sign': '„èí', 'norm': 'log', '„èí': 'log'}, {'unicode': 'U+33D3', 'sign': '„èì', 'norm': 'lx', '„èì': 'lx'}, {'unicode': 'U+33D4', 'sign': '„èî', 'norm': 'mb', '„èî': 'mb'}, {'unicode': 'U+33D5', 'sign': '„èï', 'norm': 'mil', '„èï': 'mil'}, {'unicode': 'U+33D6', 'sign': '„èñ', 'norm': 'mol', '„èñ': 'mol'}, {'unicode': 'U+33D7', 'sign': '„èó', 'norm': 'PH', '„èó': 'PH'}, {'unicode': 'U+33D9', 'sign': '„èô', 'norm': 'PPM', '„èô': 'PPM'}, {'unicode': 'U+33DA', 'sign': '„èö', 'norm': 'PR', '„èö': 'PR'}, {'unicode': 'U+33DB', 'sign': '„èõ', 'norm': 'sr', '„èõ': 'sr'}, {'unicode': 'U+33DC', 'sign': '„èú', 'norm': 'Sv', '„èú': 'Sv'}, {'unicode': 'U+33DD', 'sign': '„èù', 'norm': 'Wb', '„èù': 'Wb'}, {'unicode': 'U+33FF', 'sign': '„èø', 'norm': 'gal', '„èø': 'gal'}, {'unicode': 'U+FB00', 'sign': 'Ô¨Ä', 'norm': 'ff', 'Ô¨Ä': 'ff'}, {'unicode': 'U+FB01', 'sign': 'Ô¨Å', 'norm': 'fi', 'Ô¨Å': 'fi'}, {'unicode': 'U+FB02', 'sign': 'Ô¨Ç', 'norm': 'fl', 'Ô¨Ç': 'fl'}, {'unicode': 'U+FB03', 'sign': 'Ô¨É', 'norm': 'ffi', 'Ô¨É': 'ffi'}, {'unicode': 'U+FB04', 'sign': 'Ô¨Ñ', 'norm': 'ffl', 'Ô¨Ñ': 'ffl'}, {'unicode': 'U+FB05', 'sign': 'Ô¨Ö', 'norm': 'st', 'Ô¨Ö': 'st'}, {'unicode': 'U+FB06', 'sign': 'Ô¨Ü', 'norm': 'st', 'Ô¨Ü': 'st'}, {'unicode': 'U+1F12D', 'sign': 'üÑ≠', 'norm': 'CD', 'üÑ≠': 'CD'}, {'unicode': 'U+1F12E', 'sign': 'üÑÆ', 'norm': 'WZ', 'üÑÆ': 'WZ'}, {'unicode': 'U+1F14A', 'sign': 'üÖä', 'norm': 'HV', 'üÖä': 'HV'}, {'unicode': 'U+1F14B', 'sign': 'üÖã', 'norm': 'MV', 'üÖã': 'MV'}, {'unicode': 'U+1F14C', 'sign': 'üÖå', 'norm': 'SD', 'üÖå': 'SD'}, {'unicode': 'U+1F14D', 'sign': 'üÖç', 'norm': 'SS', 'üÖç': 'SS'}, {'unicode': 'U+1F14E', 'sign': 'üÖé', 'norm': 'PPV', 'üÖé': 'PPV'}, {'unicode': 'U+1F14F', 'sign': 'üÖè', 'norm': 'WC', 'üÖè': 'WC'}, {'unicode': 'U+1F16A', 'sign': 'üÖ™', 'norm': 'MC', 'üÖ™': 'MC'}, {'unicode': 'U+1F16B', 'sign': 'üÖ´', 'norm': 'MD', 'üÖ´': 'MD'}, {'unicode': 'U+1F16C', 'sign': 'üÖ¨', 'norm': 'MR', 'üÖ¨': 'MR'}, {'unicode': 'U+1F190', 'sign': 'üÜê', 'norm': 'DJ', 'üÜê': 'DJ'}]
ligatures_conv = {'ƒ≤': 'IJ', 'ƒ≥': 'ij', '«á': 'LJ', '«à': 'Lj', '«â': 'lj', '«ä': 'NJ', '«ã': 'Nj', '«å': 'nj', '«±': 'DZ', '«≤': 'Dz', '«≥': 'dz', '‚Ç®': 'Rs', '‚Ññ': 'No', '‚Ñ†': 'SM', '‚Ñ°': 'TEL', '‚Ñ¢': 'TM', '‚Ñª': 'FAX', '‚Ö°': 'II', '‚Ö¢': 'III', '‚Ö£': 'IV', '‚Ö•': 'VI', '‚Ö¶': 'VII', '‚Öß': 'VIII', '‚Ö®': 'IX', '‚Ö™': 'XI', '‚Ö´': 'XII', '‚Ö±': 'ii', '‚Ö≤': 'iii', '‚Ö≥': 'iv', '‚Öµ': 'vi', '‚Ö∂': 'vii', '‚Ö∑': 'viii', '‚Ö∏': 'ix', '‚Ö∫': 'xi', '‚Öª': 'xii', '„âê': 'PTE', '„ãå': 'Hg', '„ãç': 'erg', '„ãé': 'eV', '„ãè': 'LTD', '„ç±': 'hPa', '„ç≤': 'da', '„ç≥': 'AU', '„ç¥': 'bar', '„çµ': 'oV', '„ç∂': 'pc', '„ç∑': 'dm', '„ç∫': 'IU', '„éÄ': 'pA', '„éÅ': 'nA', '„éÉ': 'mA', '„éÑ': 'kA', '„éÖ': 'KB', '„éÜ': 'MB', '„éá': 'GB', '„éà': 'cal', '„éâ': 'kcal', '„éä': 'pF', '„éã': 'nF', '„éé': 'mg', '„éè': 'kg', '„éê': 'Hz', '„éë': 'kHz', '„éí': 'MHz', '„éì': 'GHz', '„éî': 'THz', '„éñ': 'ml', '„éó': 'dl', '„éò': 'kl', '„éô': 'fm', '„éö': 'nm', '„éú': 'mm', '„éù': 'cm', '„éû': 'km', '„é©': 'Pa', '„é™': 'kPa', '„é´': 'MPa', '„é¨': 'GPa', '„é≠': 'rad', '„é∞': 'ps', '„é±': 'ns', '„é≥': 'ms', '„é¥': 'pV', '„éµ': 'nV', '„é∑': 'mV', '„é∏': 'kV', '„éπ': 'MV', '„é∫': 'pW', '„éª': 'nW', '„éΩ': 'mW', '„éæ': 'kW', '„éø': 'MW', '„èÉ': 'Bq', '„èÑ': 'cc', '„èÖ': 'cd', '„èà': 'dB', '„èâ': 'Gy', '„èä': 'ha', '„èã': 'HP', '„èå': 'in', '„èç': 'KK', '„èé': 'KM', '„èè': 'kt', '„èê': 'lm', '„èë': 'ln', '„èí': 'log', '„èì': 'lx', '„èî': 'mb', '„èï': 'mil', '„èñ': 'mol', '„èó': 'PH', '„èô': 'PPM', '„èö': 'PR', '„èõ': 'sr', '„èú': 'Sv', '„èù': 'Wb', '„èø': 'gal', 'Ô¨Ä': 'ff', 'Ô¨Å': 'fi', 'Ô¨Ç': 'fl', 'Ô¨É': 'ffi', 'Ô¨Ñ': 'ffl', 'Ô¨Ö': 'st', 'Ô¨Ü': 'st', 'üÑ≠': 'CD', 'üÑÆ': 'WZ', 'üÖä': 'HV', 'üÖã': 'MV', 'üÖå': 'SD', 'üÖç': 'SS', 'üÖé': 'PPV', 'üÖè': 'WC', 'üÖ™': 'MC', 'üÖ´': 'MD', 'üÖ¨': 'MR', 'üÜê': 'DJ'}
ligatures_list = ['ƒ≤', 'ƒ≥', '«á', '«à', '«â', '«ä', '«ã', '«å', '«±', '«≤', '«≥', '‚Ç®', '‚Ññ', '‚Ñ†', '‚Ñ°', '‚Ñ¢', '‚Ñª', '‚Ö°', '‚Ö¢', '‚Ö£', '‚Ö•', '‚Ö¶', '‚Öß', '‚Ö®', '‚Ö™', '‚Ö´', '‚Ö±', '‚Ö≤', '‚Ö≥', '‚Öµ', '‚Ö∂', '‚Ö∑', '‚Ö∏', '‚Ö∫', '‚Öª', '„âê', '„ãå', '„ãç', '„ãé', '„ãè', '„ç±', '„ç≤', '„ç≥', '„ç¥', '„çµ', '„ç∂', '„ç∑', '„ç∫', '„éÄ', '„éÅ', '„éÉ', '„éÑ', '„éÖ', '„éÜ', '„éá', '„éà', '„éâ', '„éä', '„éã', '„éé', '„éè', '„éê', '„éë', '„éí', '„éì', '„éî', '„éñ', '„éó', '„éò', '„éô', '„éö', '„éú', '„éù', '„éû', '„é©', '„é™', '„é´', '„é¨', '„é≠', '„é∞', '„é±', '„é≥', '„é¥', '„éµ', '„é∑', '„é∏', '„éπ', '„é∫', '„éª', '„éΩ', '„éæ', '„éø', '„èÉ', '„èÑ', '„èÖ', '„èà', '„èâ', '„èä', '„èã', '„èå', '„èç', '„èé', '„èè', '„èê', '„èë', '„èí', '„èì', '„èî', '„èï', '„èñ', '„èó', '„èô', '„èö', '„èõ', '„èú', '„èù', '„èø', 'Ô¨Ä', 'Ô¨Å', 'Ô¨Ç', 'Ô¨É', 'Ô¨Ñ', 'Ô¨Ö', 'Ô¨Ü', 'üÑ≠', 'üÑÆ', 'üÖä', 'üÖã', 'üÖå', 'üÖç', 'üÖé', 'üÖè', 'üÖ™', 'üÖ´', 'üÖ¨', 'üÜê']
pattern = "ƒ≤|ƒ≥|«á|«à|«â|«ä|«ã|«å|«±|«≤|«≥|‚Ç®|‚Ññ|‚Ñ†|‚Ñ°|‚Ñ¢|‚Ñª|‚Ö°|‚Ö¢|‚Ö£|‚Ö•|‚Ö¶|‚Öß|‚Ö®|‚Ö™|‚Ö´|‚Ö±|‚Ö≤|‚Ö≥|‚Öµ|‚Ö∂|‚Ö∑|‚Ö∏|‚Ö∫|‚Öª|„âê|„ãå|„ãç|„ãé|„ãè|„ç±|„ç≤|„ç≥|„ç¥|„çµ|„ç∂|„ç∑|„ç∫|„éÄ|„éÅ|„éÉ|„éÑ|„éÖ|„éÜ|„éá|„éà|„éâ|„éä|„éã|„éé|„éè|„éê|„éë|„éí|„éì|„éî|„éñ|„éó|„éò|„éô|„éö|„éú|„éù|„éû|„é©|„é™|„é´|„é¨|„é≠|„é∞|„é±|„é≥|„é¥|„éµ|„é∑|„é∏|„éπ|„é∫|„éª|„éΩ|„éæ|„éø|„èÉ|„èÑ|„èÖ|„èà|„èâ|„èä|„èã|„èå|„èç|„èé|„èè|„èê|„èë|„èí|„èì|„èî|„èï|„èñ|„èó|„èô|„èö|„èõ|„èú|„èù|„èø|Ô¨Ä|Ô¨Å|Ô¨Ç|Ô¨É|Ô¨Ñ|Ô¨Ö|Ô¨Ü|üÑ≠|üÑÆ|üÖä|üÖã|üÖå|üÖç|üÖé|üÖè|üÖ™|üÖ´|üÖ¨|üÜê"



def pdf2html(target="./SAMPLE/NCLIMATE/s41558-020-00938-y_Heat_Tolerance_In_Ectotherms_Scales_Predictably_With_Body_Size_.pdf", line_margin=0.5, all_texts=True):
    """
    Converts a PDF file to HTML format.

    Args:
        target (str): Path to the PDF file to be converted. Default is "./SAMPLE/NCLIMATE/s41558-020-00938-y_Heat_Tolerance_In_Ectotherms_Scales_Predictably_With_Body_Size_.pdf".
        line_margin (float): Margin between lines in the output HTML. Default is 0.5.
        all_texts (bool): If True, extracts all text from the PDF. If False, only extracts text from visible elements. Default is True.

    Returns:
        str: HTML representation of the PDF content.

    Raises:
        None.

    Note:
        This function utilizes the `extract_text_to_fp` method from PyPDF2 library to extract text from the PDF.
        If the PDF cannot be read, a warning message is logged, and None is returned.

    """

    try:
        output_string = StringIO()

        with open(target, 'rb') as fin:
            extract_text_to_fp(fin, output_string, laparams=LAParams(), output_type='html', codec=None)

        return output_string.getvalue()
    except:
        warning_message = f"Unable to read PDF. Skipping..."
        logging.warning(warning_message)
        return None
    
def check_if_ium(soup):
    """
    Check if the provided BeautifulSoup 'soup' object indicates incomplete Unicode mappings (ium).

    Parameters:
    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML content.

    Returns:
    - bool: True if incomplete Unicode mappings are found, False otherwise.

    The function searches for a 'div' element in the provided 'soup' object and iterates through its
    next siblings until a text length of at least 50 characters is found or no more siblings are present.
    It then uses a regular expression to check if the text contains at least three consecutive instances
    of the pattern "(cid:\d+)" which might indicate incomplete Unicode mappings.
    """
    elem = soup.find("div")
    while len(elem.text) < 50 and not type(elem) == type(None):
        elem = elem.find_next()

    if type(elem) == type(None):
        raise Exception("NO div element found!")
    
    return bool(re.search("(\(cid:\d+\)){3,}", elem.text))

def add_custom_tag_after_element(soup, target_element, tag_name, tag_content, style_attributes):
    """
    Add a custom tag with specified content after a specific element in the BeautifulSoup 'soup' object.

    Parameters:
    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML content.
    - target_element (Tag): The target element after which the new tag will be added.
    - tag_name (str): The name of the custom tag to be added.
    - tag_content (str): The content to be placed inside the custom tag.

    Returns:
    - BeautifulSoup: The modified BeautifulSoup object.
    """
    if type(target_element) == type(None):
        warning_message = "Tag is not added correctly -> Implies that the target element wasn't found correctly prior"
        logging.warning(warning_message)
        return soup
    new_tag = soup.new_tag(tag_name)
    new_tag.string = tag_content
    new_tag.attrs.update(style_attributes)
    target_element.insert_after(new_tag)
    return soup

def find_custom_element_by_regex(soup, regex="^(?i)r\s*e\s*f\s*e\s*r\s*e\s*n\s*c\s*e\s*s\n+", reverese=True):
    """
    Finds a custom HTML element within a BeautifulSoup object based on a given regular expression.

    Args:
    - soup: BeautifulSoup object representing HTML content.
    - regex (str): Regular expression pattern to search for within the text content of HTML elements.
    - reverse (bool): If True, searches for the element in reverse order (from the last element to the first).

    Returns:
    - elem: The BeautifulSoup element that matches the specified regex pattern, or None if not found.
    """
    if reverese:
        elem = soup.find_all('div')[-1]
        while type(elem) != type(None):
            if re.search(regex, elem.text):   
                # print(elem.text)
                break
            elem = elem.find_previous()
    else:
        elem = soup.find('div')
        while type(elem) != type(None):
            if re.search(regex, elem.text):   
                # print(elem.text)
                break
            elem = elem.find_next()

    return elem

def likely_word(tokenbefore, token, tokenafter):
    """
    Check if the combination of tokens (current, before, and after) constitutes a likely word.

    Args:
        tokenbefore (str): The token preceding the current token.
        token (str): The current token.
        tokenafter (str): The token following the current token.

    Returns:
        tuple: A tuple containing the following elements:
            - str: The modified tokenbefore string, after cleaning and processing.
            - str: The modified token string, after cleaning and processing.
            - str: The modified tokenafter string, after cleaning and processing.
            - int: Flag indicating if a likely word is found (1) or not (0).
    """

    temp_token = token
    temp_tokenbefore = tokenbefore
    temp_tokenafter = tokenafter

    # Clean from punctuation and simmilar
    token = re.sub(r"[(),.!?;]+", "", token).lower()
    tokenbefore = re.sub(r"[(),.!?;]+", "", tokenbefore).lower()
    tokenafter = re.sub(r"[(),.!?;]+", "", tokenafter).lower()

    # Deal with concatenated words, such as "age-specific"
    tokenbefore = re.sub(r"\w+-", "", tokenbefore)

    replace_token = ""
    
    if tokenbefore+token+tokenafter in words.words() or lemmatizer.lemmatize(tokenbefore+token+tokenafter) in words.words() or lemmatizer.lemmatize(tokenbefore+token+tokenafter, pos='v') in words.words():
        return replace_token, temp_tokenbefore+temp_token+temp_tokenafter, replace_token, 1
    
    elif token+tokenafter in words.words() or lemmatizer.lemmatize(token+tokenafter) in words.words() or lemmatizer.lemmatize(token+tokenafter, pos='v') in words.words():
        return temp_tokenbefore, temp_token+temp_tokenafter, replace_token, 1
    
    elif tokenbefore+token in words.words() or lemmatizer.lemmatize(tokenbefore+token) in words.words() or lemmatizer.lemmatize(tokenbefore+token, pos='v') in words.words():
        return replace_token, temp_tokenbefore+temp_token, temp_tokenafter, 1
    
    elif token in words.words() in words.words():
        return temp_tokenbefore, temp_token, temp_tokenafter, 1
    
    else:
        if len(temp_token) > 2:
            return temp_tokenbefore, temp_token, temp_tokenafter, 1
        else: 
            # print(temp_tokenbefore, temp_token, temp_tokenafter)
            return temp_tokenbefore, replace_token, temp_tokenafter, 0
        
def fi_cleaner(text):
    """
    Cleans the text by replacing ligatures with their corresponding characters.
    
    Args:
    text (str): The input text to be cleaned.
    
    Returns:
    str: The cleaned text with ligatures replaced.
    """
    
    tokens = text.split()
    
    for i, word in enumerate(tokens):
        count = 0

        for j, c in enumerate(word):
            if c in ligatures_list:
                if (j == 0 or j == len(word)-1):
                    # temp = tokens[i]
                    # print(20*"-")
                    # print(i, "\t", tokens[i-1], tokens[i], tokens[i+1], end="----")
                    count += 1
                    try:
                        tokens[i-1], tokens[i], tokens[i+1], f = likely_word(tokens[i-1], re.sub(pattern, lambda m: ligatures_conv.get(m.group(0)), tokens[i]), tokens[i+1])
                    except:
                        continue
                    # if f == 0:
                    #     fi_counter_unsolved.append((temp, tokens[i]))
                    # else:
                    #     fi_counter_solved.append((temp, tokens[i]))
                else:
                    tokens[i] = re.sub(pattern, lambda m: ligatures_conv.get(m.group(0)), tokens[i])
                    count += 1
                # print(tokens[i-1], tokens[i], tokens[i+1])
        if count > 1: # Check if some word containes multiple ligatures
            print(i, "\t", tokens[i-1], tokens[i], tokens[i+1])
            warning_message = f"Multiple ligatures in single word!"
            logging.warning(warning_message)
            
    return " ".join(tokens)
    
